{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945ebf4a",
   "metadata": {},
   "source": [
    "# Как можно оценивать RAG\n",
    "\n",
    "## Есть разные варианты ошибок LLM:\n",
    "- Лингвистические (грамматика, синтаксис и т.д.) - решаются через улучшение данных на претрейне LLM\n",
    "- Этические (некорректное, предвзятое или оскорбительное поведение) - решаются через улучшение данных на претрейне LLM (***Alignment???***)\n",
    "- Фактологические (связанные с неверной или устаревшей информацией) - решаются через обновление обучающего корпуса данных(дорого), использование RAG\n",
    "\n",
    "## Как можно оценивать RAG:\n",
    "1. Оценка Retrieve части\n",
    "    1. **Retrieving метрики** - метрики ранжиронвания и классификации, которые оценивают выдачу контекста (MAP@k, NCDG@k и т.д.). **Но нужна разметка**.\n",
    "\n",
    "2. Оценка генерации\n",
    "    1. **Faithfulness (Answer~Context)** - Согласованность генеративного ответа к найденному контексту (можно считать в **онлайне** и **не нужна** разметка)\n",
    "    2. **Relevance (Answer~Query)** - Насколько сгенерированный ответ соответствует вопросу (можно считать в **онлайне** и **не нужна** разметка)\n",
    "    3. **Correctness(Answer~Answer\\*)** - Измеряет степень соответствия генеративного ответа эталонному ответу (расчет только в **офлайне** и **нужна** разметка)\n",
    "\n",
    "## Методы реализации расчета метрик:\n",
    "1. **Human Eval** - разметка ассесорами (дорого, долго, но качественно), нужен **Golden Set(валидационная выборка)**\n",
    "2. **LLM as a Judge** - разметка LLM (дешево, быстро, чаще хуже чем разметка ассесорами). Плохо следуют большому кол-ву инструкций [статья](https://openreview.net/pdf?id=R6q67CDBCH)\n",
    "3. **Детермминированные метрики** - precision, recall, F1, BLEU, ROUGE, BertScore, слабо отражают реальность\n",
    "\n",
    "## Фреймворки для оцеки RAG:\n",
    "- **RAGAS** \n",
    "\n",
    "## Дополнительно:\n",
    "- [Как оценивать современные RAG-системы?](https://youtu.be/fE3gDFi7tNQ?si=FbgW8avZEXK15vOI)\n",
    "- [Полное руководство по оценке компонентов системы RAG: что необходимо знать](https://habr.com/ru/articles/860390/)\n",
    "- [Руководство для начинающих по оценке конвейеров RAG с использованием RAGAS](https://llmarena.team/blog/rukovodstvo-dlya-nachinayushchih-po-ocenke-konvejerov-rag)\n",
    "- https://arxiv.org/pdf/2405.07437"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd360151",
   "metadata": {},
   "source": [
    "**Faithfulness (Answer~Context) Достоверность** - Согласованность генеративного ответа к найденному контексту (можно считать в **онлайне** и **не нужна** разметка) через LLM as a Judge. Необходимо сгенерить несколько ответов и отношение (кол-во ответов согласованных с контекстом) / (кол-во всех ответов), либо оценка согласованности.\n",
    "\n",
    "**Answer Relevancy** - Насколько сгенерированный ответ соответствует вопросу (можно считать в **онлайне** и **не нужна** разметка) генерируются 3 вопроса к полученному ответу и считается косинусное расстояние между вопросами и изначальным вопросом\n",
    "\n",
    "\n",
    "**Context Precision** - доля релевантных чанков контекста из всех извлеченных чанков\n",
    "**Context Recall** - доля извлечённых релевантных чанков от общего числа релевантных чанков\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe933d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66520557",
   "metadata": {},
   "source": [
    "# Как можно оценивать RAG\n",
    "\n",
    "![lllm_eval.png](../imgs/lllm_eval.png)\n",
    "\n",
    "## Мотивация\n",
    "Хочется иметь понимание насколько модификации системы/модели лучше baseline решение.\n",
    "\n",
    "P.S. Важно определить, что важнее precision(правильно отвечать на вопросы) или recall(отвечать на как можно больше вопросов)?\n",
    "\n",
    "## Есть несколько этапов, на которых можно оценивать качество RAG\n",
    "Будем рассмативать этапы с конца\n",
    "- Генерация\n",
    "- Ретрив (поиск чанков/документов)\n",
    "\n",
    "![rag_schema](../imgs/rag_schema.png)\n",
    "\n",
    "## Метрики генерации\n",
    "**Можно разбить на 2 группы:**\n",
    "- Метрики качества итогового ответа (BERTScore и т.д.)\n",
    "- Метрики качества генерации на основе контекста (Faithfulness, Groundedness)\n",
    "\n",
    "### Качество итогового ответа\n",
    "Для этих метрик нужны ground truth (эталонные) ответы. Данные метрики показывают насколько получнный ответ схож с ground truth (эталонным) ответом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e8135",
   "metadata": {},
   "source": [
    "**Можно разбить на 2 группы:**\n",
    "- Non-LLM-Based метрики\n",
    "    - BertScore\n",
    "    - Semantic Similarity\n",
    "    - BLEU (Bilingual Evaluation Understudy)\n",
    "    - ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "    - METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "- LLM-Based метрики\n",
    "    - \n",
    "\n",
    "#### Non-LLM-Based метрики\n",
    "\n",
    "##### BertScore\n",
    "Использует контекстуальные эмбеддинги, учитывает семантическое сходство отличие от BLEU или ROUGE. BERTScore позволяет выявлять эквивалентность смысла даже при различии в словах и формулировках, учитывая синонимы и парафразы. \n",
    "\n",
    "Для расчета нужны эталонные ответы и ответы от RAG/LLM/системы, чьи ответы проверяем.\n",
    "\n",
    "![bertscore](../imgs/bertscore.png)\n",
    "\n",
    "###### Методика расчета\n",
    "Метод BERTScore состоит из нескольких этапов:\n",
    "1. **Получение контекстуальных эмбеддингов:** Оба текста (референсный и сгенерированный) разбиваются на токены и пропускаются через предобученную трансформерную модель (например, BERT или RoBERTa). Для каждого токена извлекается его контекстуальное векторное представление (эмбеддинг).\n",
    "2. **Вычисление косинусного сходства:** Для всех пар токенов из двух текстов вычисляется косинусное сходство, и формируется матрица сходства токенов.\n",
    "3. **Расчёт точности, полноты и $F_1$-меры:** На основе матрицы сходства для каждого токена в сгенерированном тексте находится наиболее похожий токен в референсном тексте, что позволяет вычислить точность (precision). Аналогично, для каждого токена референса находится самый близкий токен в сгенерированном тексте, что даёт полноту (recall). Итоговым значением BERTScore является сбалансированная $F_1$-мера, которая комбинирует точность и полноту\n",
    "\n",
    "> $recall = \\frac{1}{|x|} \\sum\\limits_{x_i \\in x} \\max\\limits_{\\hat{x_j} \\in \\hat{x}} x_i^T \\hat{x_j}$\n",
    "\n",
    "> $precision = \\frac{1}{|\\hat{x}|} \\sum\\limits_{\\hat{x_j} \\in \\hat{x}} \\max\\limits_{x_i \\in x} x_i^T \\hat{x_j}$\n",
    "\n",
    "> $BertScore = F_1 = 2 * \\frac{precision * recal}{precision + recal}$\n",
    "\n",
    "4. \\* можно взвешивать токены по их важности (с помощью IDF-весов) и преобразовывать оценки от 0 до 1 для лучшей интерпретируемости (параметры в BertScorer)\n",
    "\n",
    "Допустимые значения без нормировки [-1, 1]. Чем ближе BertScore к 1 тем больше полученный ответ совпадает с эталонным ответом.\n",
    "\n",
    "###### Плюсы и минусы\n",
    "**Плюсы:**\n",
    "- **Учёт семантики:** Сравнивает тексты на уровне смысла, учитывая синонимы и парафразы.\n",
    "- **Высокая корреляция с человеком:** Оценки BERTScore лучше согласуются с человеческими суждениями о качестве текста, чем традиционные метрики\n",
    "\n",
    "**Минусы:**\n",
    "- **Вычислительные затраты:** Расчёт на основе эмбеддингов требует значительно больше ресурсов, чем подсчёт n-грамм, и часто требует использования GPU\n",
    "\n",
    "##### Semantic Similarity\n",
    "Semantic Similarity оценивает семантическое сходство между сгенерированным ответом и эталонным ответом. В отличии от **BertScore** сравнивают эмбеддинги не слов/токенов, а ответов/предложений и уже от них берется косинусное расстояние.\n",
    "\n",
    "Для расчета нужны эталонные ответы и ответы от RAG/LLM/системы, чьи ответы проверяем.\n",
    "\n",
    "###### Методика расчета\n",
    "Метод Semantic Similarity состоит из нескольких этапов:\n",
    "1. **Получение эмбеддингов:** Оба текста (референсный и сгенерированный) пропускаются через предобученную трансформерную модель. Для текстов извлекаются векторные представления (эмбеддинги).\n",
    "2. **Вычисление косинусного сходства**\n",
    "\n",
    "Допустимые значения без нормировки [-1, 1]. Чем ближе Semantic Similarity к 1 тем больше полученный ответ совпадает с эталонным ответом.\n",
    "\n",
    "P.S. В [RAGAS](https://github.com/vibrantlabsai/ragas/blob/main/src/ragas/metrics/collections/_semantic_similarity.py) почему-то написано, что от 0 до 1, хотя считают косинусное расстояние.\n",
    "\n",
    "###### Плюсы и минусы\n",
    "**Плюсы:**\n",
    "- **Учёт семантики:** Сравнивает тексты на уровне смысла, учитывая синонимы и парафразы.\n",
    "- **Высокая корреляция с человеком** \n",
    "\n",
    "**Минусы:**\n",
    "- **Вычислительные затраты:** Расчёт на основе эмбеддингов требует значительно больше ресурсов, чем подсчёт n-грамм, и часто требует использования GPU\n",
    "\n",
    "\n",
    "##### BLEU (Bilingual Evaluation Understudy)\n",
    "BLEU вычисляет точность для каждого совпадающего n-грамма между полученным ответом и эталонным ответом, чтобы найти их геометрическое среднее, и применяет штраф за краткость, если это необходимо. Изначально применялся для оценки машинного перевода.\n",
    "\n",
    "###### Методика расчета\n",
    "Метод Semantic Similarity состоит из нескольких этапов:\n",
    "1. **Получение эмбеддингов:** Оба текста (референсный и сгенерированный) пропускаются через предобученную трансформерную модель. Для текстов извлекаются векторные представления (эмбеддинги).\n",
    "2. **Вычисление косинусного сходства**\n",
    "\n",
    "Допустимые значения [0, 1]. Чем ближе BLEU к 1 тем больше полученный ответ совпадает с эталонным ответом.\n",
    "\n",
    "\n",
    "###### Плюсы и минусы\n",
    "**Плюсы:**\n",
    "- **Учёт семантики:** Сравнивает тексты на уровне смысла, учитывая синонимы и парафразы.\n",
    "- **Высокая корреляция с человеком** \n",
    "\n",
    "**Минусы:**\n",
    "- **Вычислительные затраты:** Расчёт на основе эмбеддингов требует значительно больше ресурсов, чем подсчёт n-грамм, и часто требует использования GPU\n",
    "\n",
    "\n",
    "##### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "##### METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "\n",
    "\n",
    "#### LLM-Based метрики\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Метрики ретрива\n",
    "Можно разбить на 2 группы:\n",
    "- Метрики качества релевантности контекста (Context Precision, Context Recall)\n",
    "- Метрики оценки качества ретривера (Recall@k, Precision@k, MAP@k и т.д.)\n",
    "\n",
    "\n",
    "## Фреймворки\n",
    "- [BertScore](https://github.com/Tiiiger/bert_score) - для расчета bertscore\n",
    "- [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/) - для метрик: \n",
    "    - \n",
    "- \n",
    "\n",
    "## Литература\n",
    "- [[Хабр] Полное руководство по оценке компонентов системы RAG: что необходимо знать](https://habr.com/ru/articles/860390/)\n",
    "- [BertScore arxiv](https://arxiv.org/pdf/1904.09675)\n",
    "- [BertScore github](https://github.com/Tiiiger/bert_score)\n",
    "- [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
